{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2013e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "from ptflops import get_model_complexity_info\n",
    "import math\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# BIT Architecture Implementation\n",
    "# -------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout_rate=0.1, use_dropout=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout2d(dropout_rate))\n",
    "        \n",
    "        layers.extend([\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout2d(dropout_rate))\n",
    "        \n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    \"\"\"CNN backbone encoder for feature extraction\"\"\"\n",
    "    def __init__(self, in_channels=3, features=[32, 64, 128, 256], dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # No dropout in encoder layers\n",
    "        self.enc1 = DoubleConv(in_channels, features[0], dropout_rate, use_dropout=False)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = DoubleConv(features[0], features[1], dropout_rate, use_dropout=False)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = DoubleConv(features[1], features[2], dropout_rate, use_dropout=False)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = DoubleConv(features[2], features[3], dropout_rate, use_dropout=False)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck with dropout\n",
    "        self.bottleneck = DoubleConv(features[3], features[3] * 2, dropout_rate * 2, use_dropout=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Return all intermediate features for skip connections\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "        \n",
    "        e4 = self.enc4(p3)\n",
    "        p4 = self.pool4(e4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(p4)\n",
    "        \n",
    "        return [e1, e2, e3, e4, bottleneck]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Custom transformer encoder layer with regularization\"\"\"\n",
    "    def __init__(self, d_model, nhead=8, dim_feedforward=512, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout_rate, batch_first=True)\n",
    "        \n",
    "        # Feed forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention\n",
    "        src2, _ = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # Feed forward\n",
    "        src2 = self.ffn(src)\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "\n",
    "class FeatureFusion(nn.Module):\n",
    "    \"\"\"Feature fusion module for combining features from two branches\"\"\"\n",
    "    def __init__(self, channels, dropout_rate=0.1, use_dropout=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(channels * 2, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout2d(dropout_rate))\n",
    "            \n",
    "        self.fusion_conv = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, f1, f2):\n",
    "        # Concatenate features from both branches\n",
    "        fused = torch.cat([f1, f2], dim=1)\n",
    "        return self.fusion_conv(fused)\n",
    "\n",
    "class BIT(nn.Module):\n",
    "    \"\"\"BIT (Binary Image Transformer) for change detection with regularization\"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[32, 64, 128, 256], \n",
    "                 dropout_rate=0.2, nhead=8, num_transformer_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN backbone encoder for both branches\n",
    "        self.encoder = CNNEncoder(in_channels, features, dropout_rate)\n",
    "        \n",
    "        # Transformer parameters\n",
    "        self.d_model = features[3] * 2  # 512 for bottleneck features\n",
    "        self.patch_size = 16  # Size of patches for transformer\n",
    "        \n",
    "        # Feature fusion modules for each level (no dropout in encoder feature fusion)\n",
    "        self.fusion1 = FeatureFusion(features[0], dropout_rate, use_dropout=False)\n",
    "        self.fusion2 = FeatureFusion(features[1], dropout_rate, use_dropout=False)\n",
    "        self.fusion3 = FeatureFusion(features[2], dropout_rate, use_dropout=False)\n",
    "        self.fusion4 = FeatureFusion(features[3], dropout_rate=0.1, use_dropout=True)\n",
    "        \n",
    "        # Transformer components for bottleneck features\n",
    "        self.pos_encoding = PositionalEncoding(self.d_model, dropout_rate=dropout_rate*2)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(self.d_model, nhead, dim_feedforward=self.d_model*2, \n",
    "                                  dropout_rate=dropout_rate*2)\n",
    "            for _ in range(num_transformer_layers)\n",
    "        ])\n",
    "        \n",
    "        # Projection layer for transformer output\n",
    "        self.transformer_proj = nn.Sequential(\n",
    "            nn.LayerNorm(self.d_model),\n",
    "            nn.Dropout(dropout_rate*2),\n",
    "            nn.Linear(self.d_model, self.d_model)\n",
    "        )\n",
    "        \n",
    "        # Decoder with dropout only in middle layers\n",
    "        self.up4 = nn.ConvTranspose2d(features[3] * 2, features[3], kernel_size=2, stride=2)\n",
    "        self.dec4 = DoubleConv(features[3] * 2, features[3], dropout_rate, use_dropout=True)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(features[3], features[2], kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv(features[2] * 2, features[2], dropout_rate, use_dropout=True)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(features[2], features[1], kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv(features[1] * 2, features[1], dropout_rate, use_dropout=True)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(features[1], features[0], kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv(features[0] * 2, features[0], dropout_rate, use_dropout=True)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "        \n",
    "    def create_patches(self, x):\n",
    "        \"\"\"Convert feature maps to patches for transformer input\"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        # Flatten spatial dimensions\n",
    "        patches = x.view(B, C, H * W).transpose(1, 2)  # [B, H*W, C]\n",
    "        return patches\n",
    "    \n",
    "    def patches_to_feature_map(self, patches, H, W):\n",
    "        \"\"\"Convert patches back to feature maps\"\"\"\n",
    "        B, N, C = patches.shape\n",
    "        feature_map = patches.transpose(1, 2).view(B, C, H, W)  # [B, C, H, W]\n",
    "        return feature_map\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input into pre and post images\n",
    "        pre_img = x[:, :3, :, :]   # First 3 channels\n",
    "        post_img = x[:, 3:, :, :] # Last 3 channels\n",
    "        \n",
    "        # Extract features from both branches using shared encoder\n",
    "        pre_features = self.encoder(pre_img)  # [e1, e2, e3, e4, bottleneck]\n",
    "        post_features = self.encoder(post_img)\n",
    "        \n",
    "        # Fuse features at each level (except bottleneck)\n",
    "        fused_e1 = self.fusion1(pre_features[0], post_features[0])\n",
    "        fused_e2 = self.fusion2(pre_features[1], post_features[1])\n",
    "        fused_e3 = self.fusion3(pre_features[2], post_features[2])\n",
    "        fused_e4 = self.fusion4(pre_features[3], post_features[3])\n",
    "        \n",
    "        # Process bottleneck features with transformer\n",
    "        pre_bottleneck = pre_features[4]  # [B, C, H, W]\n",
    "        post_bottleneck = post_features[4]\n",
    "        \n",
    "        B, C, H, W = pre_bottleneck.shape\n",
    "        \n",
    "        # Create patches for transformer\n",
    "        pre_patches = self.create_patches(pre_bottleneck)  # [B, H*W, C]\n",
    "        post_patches = self.create_patches(post_bottleneck)\n",
    "        \n",
    "        # Concatenate pre and post patches for joint processing\n",
    "        combined_patches = torch.cat([pre_patches, post_patches], dim=1)  # [B, 2*H*W, C]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        combined_patches = self.pos_encoding(combined_patches.transpose(0, 1)).transpose(0, 1)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        transformer_out = combined_patches\n",
    "        for layer in self.transformer_layers:\n",
    "            transformer_out = layer(transformer_out)\n",
    "        \n",
    "        # Project transformer output\n",
    "        transformer_out = self.transformer_proj(transformer_out)\n",
    "        \n",
    "        # Split back to pre and post features\n",
    "        seq_len = H * W\n",
    "        pre_transformed = transformer_out[:, :seq_len, :]  # [B, H*W, C]\n",
    "        post_transformed = transformer_out[:, seq_len:, :]\n",
    "        \n",
    "        # Convert back to feature maps\n",
    "        pre_transformed = self.patches_to_feature_map(pre_transformed, H, W)\n",
    "        post_transformed = self.patches_to_feature_map(post_transformed, H, W)\n",
    "        \n",
    "        # Fuse transformed bottleneck features\n",
    "        fused_bottleneck = torch.cat([pre_transformed, post_transformed], dim=1)\n",
    "        \n",
    "        # Decoder with skip connections using fused features\n",
    "        d4 = self.up4(fused_bottleneck)\n",
    "        d4 = torch.cat([d4, fused_e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        \n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, fused_e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, fused_e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, fused_e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        # Output (return logits, not sigmoid)\n",
    "        out = self.final_conv(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Dataset Class for Earthquake Data\n",
    "# -------------------------\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "    \"\"\"Custom dataset for earthquake damage assessment\"\"\"\n",
    "    def __init__(self, root_dir, split='train', transform=None, img_size=(256, 256)):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Set paths based on split\n",
    "        if split == 'train':\n",
    "            self.pre_dir = os.path.join(root_dir, 'train', 'A_train_aug')\n",
    "            self.post_dir = os.path.join(root_dir, 'train', 'B_train_aug')\n",
    "            self.label_dir = os.path.join(root_dir, 'train', 'label_train_aug')\n",
    "        elif split == 'val':\n",
    "            self.pre_dir = os.path.join(root_dir, 'val', 'A_val')\n",
    "            self.post_dir = os.path.join(root_dir, 'val', 'B_val')\n",
    "            self.label_dir = os.path.join(root_dir, 'val', 'label_val')\n",
    "        else:  # test\n",
    "            self.pre_dir = os.path.join(root_dir, 'test', 'A_test')\n",
    "            self.post_dir = os.path.join(root_dir, 'test', 'B_test')\n",
    "            self.label_dir = os.path.join(root_dir, 'test', 'label_test')\n",
    "        \n",
    "        # Get list of image files\n",
    "        self.image_files = sorted([f for f in os.listdir(self.pre_dir) if f.endswith('.png')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        \n",
    "        # Load images\n",
    "        pre_img = Image.open(os.path.join(self.pre_dir, img_name)).convert('RGB')\n",
    "        post_img = Image.open(os.path.join(self.post_dir, img_name)).convert('RGB')\n",
    "        label = Image.open(os.path.join(self.label_dir, img_name)).convert('L')\n",
    "        \n",
    "        # Resize images\n",
    "        pre_img = pre_img.resize(self.img_size)\n",
    "        post_img = post_img.resize(self.img_size)\n",
    "        label = label.resize(self.img_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        if self.transform:\n",
    "            pre_img = self.transform(pre_img)\n",
    "            post_img = self.transform(post_img)\n",
    "        else:\n",
    "            pre_img = transforms.ToTensor()(pre_img)\n",
    "            post_img = transforms.ToTensor()(post_img)\n",
    "        \n",
    "        label = transforms.ToTensor()(label)\n",
    "        \n",
    "        # Ensure binary labels (0 or 1)\n",
    "        label = (label > 0.5).float()\n",
    "        \n",
    "        # Concatenate pre and post images (6 channels total)\n",
    "        combined_img = torch.cat([pre_img, post_img], dim=0)\n",
    "        \n",
    "        return combined_img, label\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss doesn't improve\"\"\"\n",
    "    def __init__(self, patience=15, min_delta=1e-4, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        self.best_weights = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Loss Functions and Metrics\n",
    "# -------------------------\n",
    "\n",
    "def dice_coef(pred, target, smooth=1e-6):\n",
    "    \"\"\"Calculate dice coefficient\"\"\"\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=(2,3))\n",
    "    denom = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n",
    "    dice = (2. * intersection + smooth) / (denom + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        dice = dice_coef(pred, target, smooth=self.smooth)\n",
    "        return 1.0 - dice\n",
    "\n",
    "def combined_loss(logits, mask, bce_weight=0.6, dice_weight=0.4):\n",
    "    \"\"\"Combined BCE + Dice loss with balanced weights\"\"\"\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    bce = bce_loss(logits, mask)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    dice = DiceLoss()(probs, mask)\n",
    "    return bce_weight * bce + dice_weight * dice\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics_batch(logits, masks, thresh=0.5):\n",
    "    \"\"\"Compute metrics for a batch\"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= thresh).float()\n",
    "    preds_flat = preds.view(-1).cpu().numpy()\n",
    "    masks_flat = masks.view(-1).cpu().numpy()\n",
    "    \n",
    "    # Handle edge case where all predictions or targets are same class\n",
    "    unique_preds = np.unique(preds_flat)\n",
    "    unique_masks = np.unique(masks_flat)\n",
    "    \n",
    "    if len(unique_preds) == 1 and len(unique_masks) == 1:\n",
    "        if unique_preds[0] == unique_masks[0]:\n",
    "            # Perfect prediction\n",
    "            if unique_preds[0] == 1:\n",
    "                tp, fp, fn, tn = len(preds_flat), 0, 0, 0\n",
    "            else:\n",
    "                tp, fp, fn, tn = 0, 0, 0, len(preds_flat)\n",
    "        else:\n",
    "            # Completely wrong prediction\n",
    "            if unique_preds[0] == 1:\n",
    "                tp, fp, fn, tn = 0, len(preds_flat), 0, 0\n",
    "            else:\n",
    "                tp, fp, fn, tn = 0, 0, len(preds_flat), 0\n",
    "    else:\n",
    "        cm = confusion_matrix(masks_flat, preds_flat, labels=[0,1])\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # Handle case where only one class present\n",
    "            if cm.shape == (1, 1):\n",
    "                if unique_masks[0] == 0:\n",
    "                    tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "                else:\n",
    "                    tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, 0, 0, 0\n",
    "    \n",
    "    eps = 1e-8\n",
    "    iou = tp / (tp + fp + fn + eps)\n",
    "    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    \n",
    "    return {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "            \"iou\": float(iou), \"dice\": float(dice), \"precision\": float(precision),\n",
    "            \"recall\": float(recall), \"f1\": float(f1), \"acc\": float(acc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Training and Validation Functions\n",
    "# -------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler=None):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    agg = {\"tn\":0,\"fp\":0,\"fn\":0,\"tp\":0}\n",
    "    \n",
    "    for imgs, masks in tqdm(loader, desc=\"Train batch\"):\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = combined_loss(logits, masks)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        \n",
    "        # Compute metrics for training accuracy\n",
    "        metas = compute_metrics_batch(logits, masks)\n",
    "        for k in [\"tn\",\"fp\",\"fn\",\"tp\"]:\n",
    "            agg[k] += metas[k]\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n",
    "    train_acc = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss, train_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    \"\"\"Validate model using the loader's existing batch size\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    agg = {\"tn\":0,\"fp\":0,\"fn\":0,\"tp\":0}\n",
    "    \n",
    "    for imgs, masks in tqdm(loader, desc=\"Val batch\"):\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = combined_loss(logits, masks)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        metas = compute_metrics_batch(logits, masks)\n",
    "        for k in [\"tn\",\"fp\",\"fn\",\"tp\"]:\n",
    "            agg[k] += metas[k]\n",
    "    \n",
    "    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n",
    "    eps = 1e-8\n",
    "    iou = tp / (tp + fp + fn + eps)\n",
    "    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    metrics = {\"loss\": epoch_loss, \"iou\": iou, \"dice\": dice, \"precision\": precision,\n",
    "               \"recall\": recall, \"f1\": f1, \"acc\": acc, \n",
    "               \"confusion\": np.array([[tn, fp], [fn, tp]])}\n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4, patience=15):\n",
    "    \"\"\"Training function with early stopping and regularization\"\"\"\n",
    "    # More aggressive weight decay for regularization\n",
    "    decay, no_decay = [], []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if p.ndim == 1 or name.endswith(\".bias\"):\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        [\n",
    "            {\"params\": decay, \"weight_decay\": 1e-3},  # Increased weight decay\n",
    "            {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "        ],\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.7, patience=7, verbose=True, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=1e-4)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_ious = []\n",
    "    val_dices = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase (batch_size=6)\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "        \n",
    "        # Validation phase (batch_size=2)\n",
    "        val_metrics = validate(model, val_loader)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.6f} | Val Acc: {val_metrics['acc']:.4f} | IoU: {val_metrics['iou']:.4f} | Dice: {val_metrics['dice']:.4f} | F1: {val_metrics['f1']:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_ious.append(val_metrics['iou'])\n",
    "        val_dices.append(val_metrics['dice'])\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_metrics['acc'])\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['loss'] < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            torch.save(model.state_dict(), 'best_bit_earthquake.pth')\n",
    "            print(f\"Saved best model with validation loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        if early_stopping(val_metrics['loss'], model):\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses, val_ious, val_dices, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb65acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Model Training Pipeline\n",
    "# -------------------------\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters - identical to Siamese U-Net\n",
    "    BATCH_SIZE = 6  # Increased batch size for better gradients\n",
    "    LEARNING_RATE = 2e-4  # Reduced learning rate\n",
    "    NUM_EPOCHS = 200\n",
    "    PATIENCE = 10  # Increased patience\n",
    "    IMG_SIZE = (256, 256)\n",
    "    \n",
    "    # Simpler data transforms (no augmentation as requested)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Dataset root directory\n",
    "    root_dir = '/kaggle/input/finaldataset/earthquakeDataset'\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EarthquakeDataset(root_dir, split='train', transform=transform, img_size=IMG_SIZE)\n",
    "    val_dataset = EarthquakeDataset(root_dir, split='val', transform=transform, img_size=IMG_SIZE)\n",
    "    test_dataset = EarthquakeDataset(root_dir, split='test', transform=transform, img_size=IMG_SIZE)\n",
    "    \n",
    "    # Create data loaders - validation and test use batch_size=1 as requested\n",
    "    train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize BIT model with same architecture and regularization as Siamese U-Net\n",
    "    model = BIT(in_channels=3, out_channels=1, features=[32, 64, 128, 256], \n",
    "               dropout_rate=0.15, nhead=8, num_transformer_layers=2).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Calculate GFLOPS\n",
    "    try:\n",
    "        # Use a sample input size of (6, 256, 256) for 6-channel input\n",
    "        macs, params = get_model_complexity_info(model, (6, 256, 256), print_per_layer_stat=False, verbose=False)\n",
    "        # Convert MACs to GFLOPS (1 GFLOP = 1e9 FLOPs, 1 MAC ≈ 2 FLOPs)\n",
    "        gflops = float(macs.replace('GMac', '').replace('MMac', '').replace(' ', ''))\n",
    "        if 'GMac' in macs:\n",
    "            gflops = gflops * 2  # Convert GMac to GFLOPS\n",
    "        elif 'MMac' in macs:\n",
    "            gflops = gflops * 2 / 1000  # Convert MMac to GFLOPS\n",
    "        print(f\"Model GFLOPS: {gflops:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate GFLOPS: {e}\")\n",
    "        print(\"Make sure to install ptflops: pip install ptflops\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting BIT training...\")\n",
    "    train_losses, val_losses, val_ious, val_dices, train_accs, val_accs = train_model(\n",
    "        model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE, PATIENCE\n",
    "    )\n",
    "    \n",
    "    return model, train_losses, val_losses, val_ious, val_dices, train_accs, val_accs, test_loader\n",
    "\n",
    "# Run the training\n",
    "model, train_losses, val_losses, val_ious, val_dices, train_accs, val_accs, test_loader = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94452203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Results Visualization\n",
    "# -------------------------\n",
    "\n",
    "# Plot training curves\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_title('BIT Training and Validation Loss')\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(train_accs, label='Training Accuracy', color='blue')\n",
    "ax2.plot(val_accs, label='Validation Accuracy', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.set_title('BIT Training and Validation Accuracy')\n",
    "\n",
    "# IoU curve\n",
    "ax3.plot(val_ious, label='Validation IoU', color='green')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('IoU')\n",
    "ax3.legend()\n",
    "ax3.set_title('BIT Validation IoU')\n",
    "\n",
    "# Dice curve\n",
    "ax4.plot(val_dices, label='Validation Dice', color='orange')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Dice Coefficient')\n",
    "ax4.legend()\n",
    "ax4.set_title('BIT Validation Dice Coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bit_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11898b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Test Evaluation\n",
    "# -------------------------\n",
    "\n",
    "# Load best model for testing\n",
    "print(\"Loading best BIT model for test evaluation...\")\n",
    "model.load_state_dict(torch.load('best_bit_earthquake.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on test set (batch_size=1)\n",
    "print(\"\\nEvaluating BIT on test set...\")\n",
    "test_metrics = validate(model, test_loader)\n",
    "\n",
    "# Print test metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BIT (BINARY IMAGE TRANSFORMER) TEST EVALUATION METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test set processed with batch_size=1\")\n",
    "print(f\"Loss:            {test_metrics['loss']:.6f}\")\n",
    "print(f\"IoU:             {test_metrics['iou']:.4f}\")\n",
    "print(f\"Dice Coefficient: {test_metrics['dice']:.4f}\")\n",
    "print(f\"Accuracy:        {test_metrics['acc']:.4f}\")\n",
    "print(f\"Precision:       {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:          {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:        {test_metrics['f1']:.4f}\")\n",
    "print(\"Confusion matrix (pixel-level):\")\n",
    "print(test_metrics[\"confusion\"])\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"BIT training and evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
