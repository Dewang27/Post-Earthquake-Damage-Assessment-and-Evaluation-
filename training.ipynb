{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12744392,"sourceType":"datasetVersion","datasetId":8056287},{"sourceId":12764676,"sourceType":"datasetVersion","datasetId":8069366},{"sourceId":12766316,"sourceType":"datasetVersion","datasetId":8070438}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c51debac-2a31-4d20-b1c1-499b2e14648f","cell_type":"code","source":"\n\nimport os\nimport random\nfrom glob import glob\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n\nimport albumentations as A\n\nfrom sklearn.metrics import confusion_matrix\n\n# -------------------------\n# Config / Hyperparameters (Kaggle-friendly)\n# -------------------------\nSEED = 42\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# Point DATA_ROOT to the attached dataset under /kaggle/input\n# Example for your earlier path:\nDATA_ROOT = \"/kaggle/input/tuc-cd-new/TUC-CD_new/TUE_new\"   # <<-- change if needed\n\nA_DIR = os.path.join(DATA_ROOT, \"A_new\")\nB_DIR = os.path.join(DATA_ROOT, \"B_new\")\nMASK_DIR = os.path.join(DATA_ROOT, \"label_new\")\n\n# Save best model to working directory so it appears in Outputs\nBEST_MODEL_PATH = \"/kaggle/working/best_unet4_model_new.pth\"\nBEST_CHECKPOINT = \"/kaggle/working/best_unet4_checkpoint_new.pth\"  # optional\n\nIMG_EXT = [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.tif\"]\nIMG_SIZE = (256, 256)   # safer default on Kaggle\nBATCH_SIZE = 8\nEPOCHS = 200\nEARLY_STOPPING_PATIENCE = 10\nLR = 1e-4\nWEIGHT_DECAY = 1e-5\n\n# DataLoader niceties for Kaggle (Linux)\nNUM_WORKERS = 2\nPIN_MEMORY = True\n\n# -------------------------\n# Reproducibility\n# -------------------------\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nseed_everything()\n\n# -------------------------\n# Helper: find files (assumes aligned filenames across folders)\n# -------------------------\ndef list_images(dir_path):\n    files = []\n    for ext in IMG_EXT:\n        files.extend(glob(os.path.join(dir_path, ext)))\n    files = sorted(files)\n    return files\n\npre_list = list_images(A_DIR)\npost_list = list_images(B_DIR)\nmask_list = list_images(MASK_DIR)\n\nif not (len(pre_list) == len(post_list) == len(mask_list)):\n    raise ValueError(f\"Unequal counts: pre {len(pre_list)}, post {len(post_list)}, mask {len(mask_list)}\")\n\npairs = list(zip(pre_list, post_list, mask_list))\nprint(f\"Found {len(pairs)} triplets\")\n\n# -------------------------\n# Albumentations Transforms (use 3-channel mean/std per image)\n# -------------------------\nimagenet_mean = (0.485, 0.456, 0.406)\nimagenet_std  = (0.229, 0.224, 0.225)\n\ntrain_transform = A.Compose(\n    [\n        A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n        A.RandomCrop(IMG_SIZE[0], IMG_SIZE[1]),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.2),\n        A.Rotate(limit=20, p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=0, p=0.3),\n        A.RandomBrightnessContrast(p=0.5),\n        A.GaussianBlur(blur_limit=(3,5), p=0.2),\n        # Normalize will be applied to both 'image' (pre) and 'post_img' separately\n        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n    ],\n    additional_targets={\"post_img\": \"image\", \"mask\": \"mask\"}\n)\n\nval_transform = A.Compose(\n    [\n        A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n    ],\n    additional_targets={\"post_img\": \"image\", \"mask\": \"mask\"}\n)\n\n# -------------------------\n# Dataset\n# -------------------------\nclass ChangeDetectionDataset(Dataset):\n    def __init__(self, triplets, transform=None, photometric_independent=False):\n        self.triplets = triplets\n        self.transform = transform\n        self.photometric_independent = photometric_independent\n        self.color_aug = A.Compose([\n            A.RandomBrightnessContrast(p=0.5),\n            A.HueSaturationValue(p=0.3),\n        ])\n\n    def __len__(self):\n        return len(self.triplets)\n\n    def __getitem__(self, idx):\n        pre_path, post_path, mask_path = self.triplets[idx]\n        pre = np.array(Image.open(pre_path).convert(\"RGB\"))\n        post = np.array(Image.open(post_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"))\n\n        mask = (mask > 127).astype(\"uint8\")\n\n        if self.transform:\n            augmented = self.transform(image=pre, post_img=post, mask=mask)\n            pre = augmented[\"image\"]\n            post = augmented[\"post_img\"]\n            mask = augmented[\"mask\"]\n        else:\n            pre = np.array(Image.fromarray(pre).resize(IMG_SIZE[::-1]))\n            post = np.array(Image.fromarray(post).resize(IMG_SIZE[::-1]))\n            mask = np.array(Image.fromarray(mask).resize(IMG_SIZE[::-1]))\n\n        if self.photometric_independent:\n            pre = self.color_aug(image=pre)[\"image\"]\n            post = self.color_aug(image=post)[\"image\"]\n\n        # If albumentations Normalize applied, arrays are float32 and roughly normalized.\n        # Ensure float32 and stack pre+post into H,W,6\n        if pre.dtype == np.uint8:\n            pre = pre.astype(np.float32) / 255.0\n        if post.dtype == np.uint8:\n            post = post.astype(np.float32) / 255.0\n\n        stacked = np.concatenate([pre, post], axis=2)  # H,W,6\n        img = torch.from_numpy(stacked).permute(2,0,1).float()\n        mask = torch.from_numpy(mask).unsqueeze(0).float()  # 1,H,W\n\n        return img, mask\n\n# -------------------------\n# Create datasets and splits (60/20/20 random)\n# -------------------------\nrandom.shuffle(pairs)\nn_total = len(pairs)\nn_train = int(0.6 * n_total)\nn_val = int(0.2 * n_total)\nn_test = n_total - n_train - n_val\n\ntrain_pairs = pairs[:n_train]\nval_pairs = pairs[n_train:n_train + n_val]\ntest_pairs = pairs[n_train + n_val:]\n\nprint(f\"Split: train {len(train_pairs)}, val {len(val_pairs)}, test {len(test_pairs)}\")\n\ntrain_ds = ChangeDetectionDataset(train_pairs, transform=train_transform, photometric_independent=True)\nval_ds = ChangeDetectionDataset(val_pairs, transform=val_transform, photometric_independent=False)\ntest_ds = ChangeDetectionDataset(test_pairs, transform=val_transform, photometric_independent=False)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n\n# -------------------------\n# U-Net (custom, 4 encoders + bottleneck + 4 decoders)\n# -------------------------\nclass DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass UNet4(nn.Module):\n    def __init__(self, in_channels=6, out_channels=1, features=[64,128,256,512]):\n        super().__init__()\n        self.enc1 = DoubleConv(in_channels, features[0])\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = DoubleConv(features[0], features[1])\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = DoubleConv(features[1], features[2])\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = DoubleConv(features[2], features[3])\n        self.pool4 = nn.MaxPool2d(2)\n\n        self.bottleneck = DoubleConv(features[3], features[3]*2)\n\n        self.up4 = nn.ConvTranspose2d(features[3]*2, features[3], kernel_size=2, stride=2)\n        self.dec4 = DoubleConv(features[3]*2, features[3])\n        self.up3 = nn.ConvTranspose2d(features[3], features[2], kernel_size=2, stride=2)\n        self.dec3 = DoubleConv(features[2]*2, features[2])\n        self.up2 = nn.ConvTranspose2d(features[2], features[1], kernel_size=2, stride=2)\n        self.dec2 = DoubleConv(features[1]*2, features[1])\n        self.up1 = nn.ConvTranspose2d(features[1], features[0], kernel_size=2, stride=2)\n        self.dec1 = DoubleConv(features[0]*2, features[0])\n\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        e1 = self.enc1(x); p1 = self.pool1(e1)\n        e2 = self.enc2(p1); p2 = self.pool2(e2)\n        e3 = self.enc3(p2); p3 = self.pool3(e3)\n        e4 = self.enc4(p3); p4 = self.pool4(e4)\n        b = self.bottleneck(p4)\n        d4 = self.up4(b); d4 = torch.cat([d4, e4], dim=1); d4 = self.dec4(d4)\n        d3 = self.up3(d4); d3 = torch.cat([d3, e3], dim=1); d3 = self.dec3(d3)\n        d2 = self.up2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.dec2(d2)\n        d1 = self.up1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.dec1(d1)\n        out = self.final_conv(d1)\n        return out\n\n# -------------------------\n# Losses & Metrics\n# -------------------------\ndef dice_coef(pred, target, smooth=1e-6):\n    pred = pred.contiguous()\n    target = target.contiguous()\n    intersection = (pred * target).sum(dim=(2,3))\n    denom = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n    dice = (2. * intersection + smooth) / (denom + smooth)\n    return dice.mean()\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, pred, target):\n        dice = dice_coef(pred, target, smooth=self.smooth)\n        return 1.0 - dice\n\nbce_loss = nn.BCEWithLogitsLoss()\n\ndef combined_loss(logits, mask):\n    bce = bce_loss(logits, mask)\n    probs = torch.sigmoid(logits)\n    dice = DiceLoss()(probs, mask)\n    return bce + dice\n\n@torch.no_grad()\ndef compute_metrics_batch(logits, masks, thresh=0.5):\n    probs = torch.sigmoid(logits)\n    preds = (probs >= thresh).float()\n    preds_flat = preds.view(-1).cpu().numpy()\n    masks_flat = masks.view(-1).cpu().numpy()\n    tn, fp, fn, tp = confusion_matrix(masks_flat, preds_flat, labels=[0,1]).ravel().astype(np.int64)\n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    return {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n            \"iou\": float(iou), \"dice\": float(dice), \"precision\": float(precision),\n            \"recall\": float(recall), \"f1\": float(f1), \"acc\": float(acc)}\n\n# -------------------------\n# Training / Validation loops\n# -------------------------\ndef train_one_epoch(model, loader, optimizer):\n    model.train()\n    running_loss = 0.0\n    for imgs, masks in tqdm(loader, desc=\"Train batch\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = combined_loss(logits, masks)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * imgs.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    return epoch_loss\n\n@torch.no_grad()\ndef validate(model, loader):\n    model.eval()\n    running_loss = 0.0\n    agg = {\"tn\":0,\"fp\":0,\"fn\":0,\"tp\":0}\n    for imgs, masks in tqdm(loader, desc=\"Val batch\"):\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        logits = model(imgs)\n        loss = combined_loss(logits, masks)\n        running_loss += loss.item() * imgs.size(0)\n        metas = compute_metrics_batch(logits, masks)\n        for k in [\"tn\",\"fp\",\"fn\",\"tp\"]:\n            agg[k] += metas[k]\n    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    dice = (2 * tp) / (2 * tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps)\n    recall = tp / (tp + fn + eps)\n    f1 = 2 * precision * recall / (precision + recall + eps)\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    epoch_loss = running_loss / len(loader.dataset)\n    metrics = {\"loss\": epoch_loss, \"iou\": iou, \"dice\": dice, \"precision\": precision,\n               \"recall\": recall, \"f1\": f1, \"acc\": acc, \"confusion\": np.array([[tn, fp], [fn, tp]])}\n    return metrics\n\n# -------------------------\n# Main training\n# -------------------------\ndef main():\n    model = UNet4(in_channels=6, out_channels=1).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n    best_val_loss = float(\"inf\")\n    epochs_no_improve = 0\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_iou\": [], \"val_dice\": []}\n\n    for epoch in range(1, EPOCHS+1):\n        print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n        train_loss = train_one_epoch(model, train_loader, optimizer)\n        val_metrics = validate(model, val_loader)\n\n        print(f\"Train Loss: {train_loss:.6f}\")\n        print(f\"Val Loss: {val_metrics['loss']:.6f} | IoU: {val_metrics['iou']:.4f} | Dice: {val_metrics['dice']:.4f} | F1: {val_metrics['f1']:.4f}\")\n\n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_metrics[\"loss\"])\n        history[\"val_iou\"].append(val_metrics[\"iou\"])\n        history[\"val_dice\"].append(val_metrics[\"dice\"])\n\n        # early stopping logic + save best\n        if val_metrics[\"loss\"] < best_val_loss - 1e-6:\n            best_val_loss = val_metrics[\"loss\"]\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), BEST_MODEL_PATH)\n            # optional: save checkpoint including optimizer\n            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'best_val_loss': best_val_loss}, BEST_CHECKPOINT)\n            print(f\"Saved best model to {BEST_MODEL_PATH}.\")\n        else:\n            epochs_no_improve += 1\n            print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n\n        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n            print(f\"Early stopping triggered after {epoch} epochs (patience={EARLY_STOPPING_PATIENCE})\")\n            break\n\n    # Load best model for final evaluation on test set\n    print(\"Loading best model for test evaluation:\", BEST_MODEL_PATH)\n    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n    model.to(device)\n    test_metrics = validate(model, test_loader)\n\n    print(\"\\n--- TEST METRICS ---\")\n    print(f\"Test Loss: {test_metrics['loss']:.6f}\")\n    print(f\"IoU: {test_metrics['iou']:.4f}\")\n    print(f\"Dice: {test_metrics['dice']:.4f}\")\n    print(f\"Precision: {test_metrics['precision']:.4f}\")\n    print(f\"Recall: {test_metrics['recall']:.4f}\")\n    print(f\"F1: {test_metrics['f1']:.4f}\")\n    print(f\"Accuracy: {test_metrics['acc']:.4f}\")\n    print(\"Confusion matrix (pixel-level):\")\n    print(test_metrics[\"confusion\"])\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}