{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf40ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced U-Net (6-stage) training + validation script.\n",
    "\n",
    "Requirements:\n",
    " - PyTorch\n",
    " - Your preprocessed files:\n",
    "    - \"stacked_6channel.pt\"  -> torch.Tensor shape [N, 6, 256, 256], dtype=float32, values in [0,1]\n",
    "    - \"masks.pt\"             -> torch.Tensor shape [N, 1, 256, 256] (or [N,256,256]), values {0,1}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "# -------------------------\n",
    "# Main: load data, setup, run\n",
    "# -------------------------\n",
    "class StackedTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for stacked tensors with on-the-fly augmentation.\n",
    "    Only applies augmentation if augment=True.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x = torch.flip(x, dims=[2])\n",
    "                y = torch.flip(y, dims=[2])\n",
    "            # Random vertical flip\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x = torch.flip(x, dims=[1])\n",
    "                y = torch.flip(y, dims=[1])\n",
    "            # Random 90-degree rotation\n",
    "            if torch.rand(1) < 0.5:\n",
    "                k = random.choice([1, 2, 3])\n",
    "                x = torch.rot90(x, k, dims=[1, 2])\n",
    "                y = torch.rot90(y, k, dims=[1, 2])\n",
    "            # Random Gaussian noise\n",
    "            if torch.rand(1) < 0.3:\n",
    "                noise = torch.randn_like(x) * 0.02\n",
    "                x = torch.clamp(x + noise, 0, 1)\n",
    "        return x, y\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# -------------------------\n",
    "# Model: Enhanced U-Net (6 encoder stages)\n",
    "# -------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, mid_ch=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        if not mid_ch:\n",
    "            mid_ch = out_ch\n",
    "        layers = [\n",
    "            nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout and dropout > 0.0:\n",
    "            # place dropout after the conv stack\n",
    "            layers.append(nn.Dropout2d(p=dropout))\n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # in_ch: channels of the previous decoder feature\n",
    "        # skip_ch: channels from encoder for concat\n",
    "        # out_ch: desired output channels\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(out_ch + skip_ch, out_ch, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # handle size mismatch (if any) by padding\n",
    "        if x.size() != skip.size():\n",
    "            x = nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class EnhancedUNet6(nn.Module):\n",
    "    def __init__(self, in_channels=6, out_channels=1, base_feats=32):\n",
    "        super().__init__()\n",
    "        f = [base_feats, base_feats*2, base_feats*4, base_feats*8, base_feats*16, base_feats*16]  # six stages\n",
    "\n",
    "        # Encoder (no pooling for the first conv)\n",
    "        self.inc = DoubleConv(in_channels, f[0], dropout=0.0)\n",
    "        self.down1 = Down(f[0], f[1], dropout=0.0)\n",
    "        self.down2 = Down(f[1], f[2], dropout=0.0)\n",
    "        # add dropout in deeper encoder layers to prevent overfitting\n",
    "        self.down3 = Down(f[2], f[3], dropout=0.2)\n",
    "        self.down4 = Down(f[3], f[4], dropout=0.3)\n",
    "        self.down5 = Down(f[4], f[5], dropout=0.3)\n",
    "\n",
    "        # Bottleneck (extra conv)\n",
    "        self.bottleneck = DoubleConv(f[5], f[5], dropout=0.5)\n",
    "\n",
    "        # Decoder (mirrors encoder)\n",
    "        self.up5 = Up(f[5], f[4], f[4], dropout=0.3)\n",
    "        self.up4 = Up(f[4], f[3], f[3], dropout=0.2)\n",
    "        self.up3 = Up(f[3], f[2], f[2], dropout=0.1)\n",
    "        self.up2 = Up(f[2], f[1], f[1], dropout=0.0)\n",
    "        self.up1 = Up(f[1], f[0], f[0], dropout=0.0)\n",
    "\n",
    "        # Final conv\n",
    "        self.outc = nn.Conv2d(f[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.inc(x)     # 256\n",
    "        e2 = self.down1(e1)  # 128\n",
    "        e3 = self.down2(e2)  # 64\n",
    "        e4 = self.down3(e3)  # 32\n",
    "        e5 = self.down4(e4)  # 16\n",
    "        e6 = self.down5(e5)  # 8\n",
    "\n",
    "        b = self.bottleneck(e6)  # 8\n",
    "\n",
    "        d5 = self.up5(b, e5)  # up to 16\n",
    "        d4 = self.up4(d5, e4) # up to 32\n",
    "        d3 = self.up3(d4, e3) # up to 64\n",
    "        d2 = self.up2(d3, e2) # up to 128\n",
    "        d1 = self.up1(d2, e1) # up to 256\n",
    "\n",
    "        out = self.outc(d1)\n",
    "        return out  # logits (no sigmoid)\n",
    "\n",
    "# -------------------------\n",
    "# Losses & Metrics\n",
    "# -------------------------\n",
    "def dice_loss(probs, target, smooth=1e-6):\n",
    "    # probs & target: B x 1 x H x W\n",
    "    probs = probs.contiguous().view(probs.shape[0], -1)\n",
    "    target = target.contiguous().view(target.shape[0], -1)\n",
    "    intersection = (probs * target).sum(dim=1)\n",
    "    denom = probs.sum(dim=1) + target.sum(dim=1)\n",
    "    dice = (2.0 * intersection + smooth) / (denom + smooth)\n",
    "    return 1.0 - dice.mean()\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits\n",
    "        probs = torch.sigmoid(logits)\n",
    "        bce = self.bce(logits, targets)\n",
    "        d = dice_loss(probs, targets)\n",
    "        return bce + d\n",
    "\n",
    "def iou_score(probs, targets, thr=0.5, eps=1e-6):\n",
    "    # probs & targets: B x 1 x H x W\n",
    "    preds = (probs >= thr).float()\n",
    "    preds = preds.view(preds.shape[0], -1)\n",
    "    targets = targets.view(targets.shape[0], -1)\n",
    "    intersection = (preds * targets).sum(dim=1)\n",
    "    union = preds.sum(dim=1) + targets.sum(dim=1) - intersection\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou.mean().item()\n",
    "\n",
    "# -------------------------\n",
    "# Training / Validation loop\n",
    "# -------------------------\n",
    "def train_val_loop(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=50,\n",
    "    patience=10,\n",
    "    lr=1e-4,\n",
    "    save_path=\"best_enhanced_unet.pth\",\n",
    "):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = BCEDiceLoss()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    trigger = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} - Train\", leave=False)\n",
    "        for xb, yb in pbar:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)                    # shape B x 1 x H x W\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": f\"{train_loss / (pbar.n+1):.4f}\"})\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                probs = torch.sigmoid(logits)\n",
    "                val_iou += iou_score(probs.detach().cpu(), yb.detach().cpu())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_iou /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val IoU: {val_iou:.4f}\")\n",
    "\n",
    "        # --- Early stopping & save best ---\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            trigger = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"  -> Best model saved.\")\n",
    "        else:\n",
    "            trigger += 1\n",
    "            print(f\"  -> No improvement (trigger {trigger}/{patience})\")\n",
    "            if trigger >= patience:\n",
    "                print(\"Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "# -------------------------\n",
    "# Main: load data, setup, run\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths (change if needed)\n",
    "    stacked_path = \"stacked_6channel.pt\"\n",
    "    masks_path = \"masks.pt\"\n",
    "\n",
    "    assert os.path.exists(stacked_path), f\"{stacked_path} not found.\"\n",
    "    assert os.path.exists(masks_path), f\"{masks_path} not found.\"\n",
    "\n",
    "    stacked = torch.load(stacked_path)  # expected [N,6,256,256]\n",
    "    masks = torch.load(masks_path)      # expected [N,1,256,256] or [N,256,256]\n",
    "\n",
    "    # normalize types & shapes\n",
    "    stacked = stacked.float()\n",
    "    masks = masks.float()\n",
    "    if masks.dim() == 3:  # [N,H,W] -> [N,1,H,W]\n",
    "        masks = masks.unsqueeze(1)\n",
    "\n",
    "    # ensure binary masks\n",
    "    masks = (masks >= 0.5).float()\n",
    "\n",
    "    N = stacked.shape[0]\n",
    "    print(f\"Loaded: stacked {stacked.shape}, masks {masks.shape}, N={N}\")\n",
    "\n",
    "    # Random split: 60% train, 20% val, 20% test\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    n_train = int(0.6 * N)\n",
    "    n_val = int(0.2 * N)\n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:n_train+n_val]\n",
    "    test_idx = indices[n_train+n_val:]\n",
    "\n",
    "    X_train, y_train = stacked[train_idx], masks[train_idx]\n",
    "    X_val, y_val = stacked[val_idx], masks[val_idx]\n",
    "    X_test, y_test = stacked[test_idx], masks[test_idx]\n",
    "\n",
    "    print(f\"Split -> train: {X_train.shape[0]}, val: {X_val.shape[0]}, test: {X_test.shape[0]}\")\n",
    "\n",
    "\n",
    "    # Create DataLoaders with augmentation for training set\n",
    "    batch_size = 4   # change to 6 if GPU mem allows\n",
    "    train_loader = DataLoader(StackedTensorDataset(X_train, y_train, augment=True), batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(StackedTensorDataset(X_val, y_val, augment=False), batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(StackedTensorDataset(X_test, y_test, augment=False), batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Instantiate model\n",
    "    model = EnhancedUNet6(in_channels=6, out_channels=1, base_feats=32).to(device)\n",
    "\n",
    "    # Train with early stopping (patience=10)\n",
    "    train_val_loop(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        epochs=50,\n",
    "        patience=10,\n",
    "        lr=1e-4,\n",
    "        save_path=\"best_enhanced_unet.pth\",\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
